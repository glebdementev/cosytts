#!/usr/bin/env python3
"""
FastCosyVoice3 TTS - Non-streaming (offline) inference with metrics measurement

Uses FastCosyVoice3 with TensorRT acceleration:
- LLM: TensorRT-LLM (~3x speedup) or PyTorch with torch.compile
- Flow: TensorRT (~2.5x speedup)
- Hift: PyTorch (f0_predictor on CPU)

Non-streaming mode generates all speech tokens first, then converts to audio.
This has higher latency but can be simpler for batch processing.

Metrics:
- RTF (Real-Time Factor): synthesis_time / audio_duration (< 1.0 = faster than real-time)
- Final audio duration
- Total generation time
"""

import sys
import time
import os
import logging
import wave
from pathlib import Path

sys.path.append('third_party/Matcha-TTS')

import torch
from fastcosyvoice import FastCosyVoice3


# Optimization for torch.compile (if used)
torch.set_float32_matmul_precision('high')

# Logger configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

# Model directory
MODEL_DIR = 'pretrained_models/Fun-CosyVoice3-0.5B'

# Reference audio file (3-10 sec, clean recording)
REFERENCE_AUDIO = 'refs/audio.wav'

# Output directory
OUTPUT_DIR = 'output/run_offline'

# Instruction for the model
INSTRUCTION = "You are a helpful assistant."

# TensorRT settings
USE_TRT_FLOW = True       # TensorRT for Flow decoder (~2.5x speedup)
USE_TRT_LLM = True        # TensorRT-LLM for LLM (~3x speedup)
TRT_LLM_DTYPE = 'bfloat16'  # bfloat16/float16/float32
# Max tokens in KV-cache. 8192 tokens ‚âà 100MB for Qwen2-0.5B.
# Minimum needed: max_input_len + max_output_len = 1024 + 2048 = 3072 tokens.
TRT_LLM_KV_CACHE_TOKENS = 8192

# Inference wrapper without autograd (reduces allocations and graph leak risk)
USE_INFERENCE_MODE = True

# Text for synthesis
SYNTHESIS_TEXT = """
–ù–∞—á–∞–ª–æ –µ—Å—Ç—å –≤—Ä–µ–º—è, –∫–æ–≥–¥–∞ —Å–ª–µ–¥—É–µ—Ç –ø–æ–∑–∞–±–æ—Ç–∏—Ç—å—Å—è –æ —Ç–æ–º, —á—Ç–æ–±—ã –≤—Å–µ –±—ã–ª–æ –æ—Ç–º–µ—Ä–µ–Ω–æ –∏ —É—Ä–∞–≤–Ω–æ–≤–µ—à–µ–Ω–æ. –≠—Ç–æ –∑–Ω–∞–µ—Ç –∫–∞–∂–¥–∞—è —Å–µ—Å—Ç—Ä–∞ –ë–µ–Ω–µ –ì–µ—Å—Å–µ—Ä–∏—Ç. –ò—Ç–∞–∫, –ø—Ä–∏—Å—Ç—É–ø–∞—è –∫ –∏–∑—É—á–µ–Ω–∏—é –∂–∏–∑–Ω–∏ –ú—É–∞–¥'–î–∏–±–∞, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –≤—Ä–µ–º—è –µ–≥–æ: —Ä–æ–∂–¥–µ–Ω –≤ –ø—è—Ç—å–¥–µ—Å—è—Ç —Å–µ–¥—å–º–æ–π –≥–æ–¥ –ø—Ä–∞–≤–ª–µ–Ω–∏—è –ü–∞–¥–∏—à–∞—Ö-–ò–º–ø–µ—Ä–∞—Ç–æ—Ä–∞ –®–∞–¥–¥–∞–º–∞ IV. –ò —Å –æ—Å–æ–±—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –æ—Ç–Ω–µ—Å–∏—Ç–µ—Å—å –∫ –µ–≥–æ –º–µ—Å—Ç—É –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: –ø–ª–∞–Ω–µ—Ç–µ –ê—Ä—Ä–∞–∫–∏—Å. –ü—É—Å—Ç—å –Ω–µ —Å–º—É—Ç–∏—Ç –≤–∞—Å —Ç–æ, —á—Ç–æ —Ä–æ–¥–∏–ª—Å—è –æ–Ω –Ω–∞ –ö–∞–ª–∞–¥–∞–Ω–µ –∏ –ø–µ—Ä–≤—ã–µ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å –ª–µ—Ç —Å–≤–æ–µ–π –∂–∏–∑–Ω–∏ –ø—Ä–æ–≤–µ–ª –Ω–∞ —ç—Ç–æ–π –ø–ª–∞–Ω–µ—Ç–µ: –ê—Ä—Ä–∞–∫–∏—Å, —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞–µ–º–æ–π —Ç–∞–∫–∂–µ –î—é–Ω–æ–π, ‚Äì –≤–æ—Ç –º–µ—Å—Ç–æ –ú—É–∞–¥'–î–∏–±–∞, –≤–æ–≤–µ–∫–∏.

–ò–∑ —É—á–µ–±–Ω–∏–∫–∞ ¬´–ñ–∏–∑–Ω—å –ú—É–∞–¥'–î–∏–±–∞¬ª –ø—Ä–∏–Ω—Ü–µ—Å—Å—ã –ò—Ä—É–ª–∞–Ω

–ó–∞ –Ω–µ–¥–µ–ª—é –¥–æ –æ—Ç–ª–µ—Ç–∞ –Ω–∞ –ê—Ä—Ä–∞–∫–∏—Å, –∫–æ–≥–¥–∞ —Å—É–µ—Ç–∞ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–π –∏ —Å–±–æ—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–ª–∞ –∞–ø–æ–≥–µ—è, –ø—Ä–µ–≤—Ä–∞—Ç–∏–≤—à–∏—Å—å –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –±–µ–∑—É–º–∏–µ, –∫–∞–∫–∞—è-—Ç–æ —Å–º–æ—Ä—â–µ–Ω–Ω–∞—è —Å—Ç–∞—Ä—É—Ö–∞ –ø—Ä–∏—à–ª–∞ –∫ –º–∞—Ç–µ—Ä–∏ –ü–∞—É–ª—è.

–ù–∞–¥ –∑–∞–º–∫–æ–º –ö–∞–ª–∞–¥–∞–Ω —Å—Ç–æ—è–ª–∞ —Ç–µ–ø–ª–∞—è –Ω–æ—á—å, –Ω–æ –∏–∑ –¥—Ä–µ–≤–Ω–∏—Ö –∫–∞–º–µ–Ω–Ω—ã—Ö —Å—Ç–µ–Ω, –¥–≤–∞–¥—Ü–∞—Ç—å —à–µ—Å—Ç—å –ø–æ–∫–æ–ª–µ–Ω–∏–π —Å–ª—É–∂–∏–≤—à–∏—Ö —Ä–æ–¥—É –ê—Ç—Ä–µ–π–¥–µ—Å–æ–≤, –∫–∞–∫ –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥ —Å–º–µ–Ω–æ–π –ø–æ–≥–æ–¥—ã, –≤—ã—Å—Ç—É–ø–∏–ª —Ç–æ–Ω–∫–∏–π, –ø—Ä–æ—Ö–ª–∞–¥–Ω—ã–π –Ω–∞–ª–µ—Ç –≤–ª–∞–≥–∏.

–°—Ç–∞—Ä—É—Ö—É –≤–ø—É—Å—Ç–∏–ª–∏ —á–µ—Ä–µ–∑ –±–æ–∫–æ–≤—É—é –¥–≤–µ—Ä—å, –ø—Ä–æ–≤–µ–ª–∏ —Å–≤–æ–¥—á–∞—Ç—ã–º –∫–æ—Ä–∏–¥–æ—Ä–æ–º –º–∏–º–æ –∫–æ–º–Ω–∞—Ç—ã –ü–∞—É–ª—è, –∏ –æ–Ω–∞, –∑–∞–≥–ª—è–Ω—É–≤ –≤ –Ω–µ–µ, —É–≤–∏–¥–µ–ª–∞ –ª–µ–∂–∞—â–µ–≥–æ –≤ –ø–æ—Å—Ç–µ–ª–∏ —é–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞.

–í —Ç—É—Å–∫–ª–æ–º —Å–≤–µ—Ç–µ –ø–ª–∞–≤–∞—é—â–µ–π –ª–∞–º–ø—ã, –ø—Ä–∏—Ç—É—à–µ–Ω–Ω–æ–π –∏ –≤–∏—Å—è—â–µ–π –≤ —Å–∏–ª–æ–≤–æ–º –ø–æ–ª–µ —É —Å–∞–º–æ–≥–æ –ø–æ–ª–∞, –ø—Ä–æ—Å–Ω—É–≤—à–∏–π—Å—è –º–∞–ª—å—á–∏–∫ —É–≤–∏–¥–µ–ª –≤ –¥–≤–µ—Ä—è—Ö –≥—Ä—É–∑–Ω—É—é –∂–µ–Ω—â–∏–Ω—É ‚Äì —Ç–∞ —Å—Ç–æ—è–ª–∞ –Ω–∞ —à–∞–≥ –≤–ø–µ—Ä–µ–¥–∏ –µ–≥–æ –º–∞—Ç–µ—Ä–∏. –°—Ç–∞—Ä—É—Ö–∞ –ø–æ—Ö–æ–¥–∏–ª–∞ –Ω–∞ –≤–µ–¥—å–º—É: —Å–≤–∞–ª—è–≤—à–∞—è—Å—è –ø–∞—É—Ç–∏–Ω–∞ –≤–æ–ª–æ—Å, –ø–æ–¥–æ–±–Ω–æ –∫–∞–ø—é—à–æ–Ω—É, –∑–∞—Ç–µ–Ω—è–ª–∞ –ª–∏—Ü–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —è—Ä–∫–æ —Å–≤–µ—Ä–∫–∞–ª–∏ –≥–ª–∞–∑–∞.

‚Äì –ù–µ –º–∞–ª–æ–≤–∞—Ç –ª–∏ –æ–Ω –¥–ª—è —Å–≤–æ–∏—Ö –ª–µ—Ç, –î–∂–µ—Å—Å–∏–∫–∞? ‚Äì —Å–ø—Ä–æ—Å–∏–ª–∞ —Å—Ç–∞—Ä—É—Ö–∞. –£ –Ω–µ–µ –±—ã–ª–∞ –æ–¥—ã—à–∫–∞, –∞ —Ä–µ–∑–∫–∏–π, –¥—Ä–µ–±–µ–∑–∂–∞—â–∏–π –≥–æ–ª–æ—Å –∑–≤—É—á–∞–ª –∫–∞–∫ —Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –±–∞–ª–∏—Å–µ—Ç.

–ú–∞—Ç—å –ü–∞—É–ª—è –æ—Ç–≤–µ—Ç–∏–ª–∞ —Å–≤–æ–∏–º –º—è–≥–∫–∏–º –∫–æ–Ω—Ç—Ä–∞–ª—å—Ç–æ:

‚Äì –í—Å–µ –ê—Ç—Ä–µ–π–¥–µ—Å—ã –≤–∑—Ä–æ—Å–ª–µ—é—Ç –ø–æ–∑–¥–Ω–æ, –ü—Ä–µ–ø–æ–¥–æ–±–Ω–∞—è.

‚Äì –°–ª—ã—Ö–∞–ª–∞, ‚Äì –ø—Ä–æ—Å–∫—Ä–∏–ø–µ–ª–∞ —Å—Ç–∞—Ä—É—Ö–∞. ‚Äì –ù–æ –µ–º—É —É–∂–µ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å.

‚Äì –î–∞, –ü—Ä–µ–ø–æ–¥–æ–±–Ω–∞—è.

‚Äì –ê–≥–∞, –æ–Ω –ø—Ä–æ—Å–Ω—É–ª—Å—è –∏ —Å–ª—É—à–∞–µ—Ç! ‚Äì –°—Ç–∞—Ä—É—Ö–∞ –≤—Å–º–æ—Ç—Ä–µ–ª–∞—Å—å –≤ –ª–∏—Ü–æ –º–∞–ª—å—á–∏–∫–∞. ‚Äì –ü—Ä–∏—Ç–≤–æ—Ä—è–µ—Ç—Å—è, –º–∞–ª–µ–Ω—å–∫–∏–π —Ö–∏—Ç—Ä–µ—Ü! –ù—É –¥–∞, –¥–ª—è –ø—Ä–∞–≤–∏—Ç–µ–ª—è —Ö–∏—Ç—Ä–æ—Å—Ç—å –Ω–µ –ø–æ—Ä–æ–∫‚Ä¶ –ê –µ—Å–ª–∏ –æ–Ω –∏ –≤–ø—Ä—è–º—å –ö–≤–∏—Å–∞—Ç—Ü –•–∞–¥–µ—Ä–∞—Ö ‚Äì —Ç–æ–≥–¥–∞‚Ä¶ –≤–ø—Ä–æ—á–µ–º, –ø–æ—Å–º–æ—Ç—Ä–∏–º.

–ü–∞—É–ª—å, —É–∫—Ä—ã–≤—à–∏—Å—å –≤ —Ç–µ–Ω–∏ —Å–≤–æ–µ–≥–æ –ª–æ–∂–∞, —Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –Ω–µ–µ —Å–∫–≤–æ–∑—å –ø—Ä–∏–∫—Ä—ã—Ç—ã–µ –≤–µ–∫–∏. –ï–º—É –∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –¥–≤–∞ —Å–≤–µ—Ä–∫–∞—é—â–∏—Ö –æ–≤–∞–ª–∞ ‚Äì –≥–ª–∞–∑–∞ —Å—Ç–∞—Ä—É—Ö–∏ ‚Äì —É–≤–µ–ª–∏—á–∏–ª–∏—Å—å –∏ –∑–∞—Å–∏—è–ª–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å–≤–µ—Ç–æ–º, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏—Å—å —Å –µ–≥–æ –≤–∑–≥–ª—è–¥–æ–º.

‚Äì –°–ø–∏, —Å–ø–∏ –ø–æ–∫–∞ —Å–ø–æ–∫–æ–π–Ω–æ, –ø—Ä–∏—Ç–≤–æ—Ä—â–∏–∫, ‚Äì –ø—Ä–æ–≥–æ–≤–æ—Ä–∏–ª–∞ —Å—Ç–∞—Ä—É—Ö–∞. ‚Äì –í—ã—Å–ø–∏—Å—å –∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç: –∑–∞–≤—Ç—Ä–∞ —Ç–µ–±–µ –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –≤—Å–µ —Å–∏–ª—ã, –∫–∞–∫–∏–µ —É —Ç–µ–±—è –µ—Å—Ç—å‚Ä¶ —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å –º–æ–π –≥–æ–º –¥–∂–∞–±–±–∞—Ä‚Ä¶

–° —ç—Ç–∏–º –æ–Ω–∞ –∏ —É–¥–∞–ª–∏–ª–∞—Å—å, –≤—ã—Ç–µ—Å–Ω–∏–≤ –º–∞—Ç—å –ü–∞—É–ª—è –≤ –∫–æ—Ä–∏–¥–æ—Ä, –∏ –∑–∞—Ö–ª–æ–ø–Ω—É–ª–∞ –¥–≤–µ—Ä—å.

–ü–∞—É–ª—å –ª–µ–∂–∞–ª –∏ –¥—É–º–∞–ª. –ß—Ç–æ —Ç–∞–∫–æ–µ –≥–æ–º –¥–∂–∞–±–±–∞—Ä?

–°—Ç–∞—Ä—É—Ö–∞ –±—ã–ª–∞ —Å–∞–º—ã–º —Å—Ç—Ä–∞–Ω–Ω—ã–º –∏–∑ –≤—Å–µ–≥–æ, —á—Ç–æ –æ–Ω –≤–∏–¥–µ–ª –∑–∞ —ç—Ç–∏ –¥–Ω–∏ –ø–µ—Ä–µ–º–µ–Ω –∏ —Å—É–µ—Ç—ã —Å–±–æ—Ä–æ–≤.

–ü—Ä–µ–ø–æ–¥–æ–±–Ω–∞—è‚Ä¶

–≠—Ç–∞ ¬´–ü—Ä–µ–ø–æ–¥–æ–±–Ω–∞—è¬ª –Ω–∞–∑—ã–≤–∞–ª–∞ –µ–≥–æ –º–∞—Ç—å –ø—Ä–æ—Å—Ç–æ ¬´–î–∂–µ—Å—Å–∏–∫–∞¬ª, —Å–ª–æ–≤–Ω–æ –ø—Ä–æ—Å—Ç—É—é —Å–ª—É–∂–∞–Ω–∫—É. –ê –≤–µ–¥—å –µ–≥–æ –º–∞—Ç—å ‚Äì –ë–µ–Ω–µ –ì–µ—Å—Å–µ—Ä–∏—Ç, –ª–µ–¥–∏, –Ω–∞–ª–æ–∂–Ω–∏—Ü–∞ –≥–µ—Ä—Ü–æ–≥–∞ –õ–µ—Ç–æ –ê—Ç—Ä–µ–π–¥–µ—Å–∞, —Ä–æ–¥–∏–≤—à–∞—è –µ–º—É –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞!

–ù–æ –≥–æ–º –¥–∂–∞–±–±–∞—Ä‚Ä¶ —á—Ç–æ —ç—Ç–æ? –ù–µ—á—Ç–æ —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å –ê—Ä—Ä–∞–∫–∏—Å–æ–º? –ß—Ç–æ-—Ç–æ, —á—Ç–æ –æ–Ω –¥–æ–ª–∂–µ–Ω —É–∑–Ω–∞—Ç—å –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å—Å—è —Ç—É–¥–∞?

–û–Ω –±–µ–∑–∑–≤—É—á–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏–ª —ç—Ç–∏ —Å—Ç—Ä–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞: ¬´–≥–æ–º –¥–∂–∞–±–±–∞—Ä¬ª, ¬´–ö–≤–∏—Å–∞—Ç—Ü –•–∞–¥–µ—Ä–∞—Ö¬ª‚Ä¶

–ü—Ä–µ–¥—Å—Ç–æ—è–ª–æ —É–∑–Ω–∞—Ç—å —Å—Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ–≥–æ! –ê—Ä—Ä–∞–∫–∏—Å —Ç–∞–∫ –æ—Ç–ª–∏—á–∞–ª—Å—è –æ—Ç –ö–∞–ª–∞–¥–∞–Ω–∞, —á—Ç–æ –≥–æ–ª–æ–≤–∞ –ü–∞—É–ª—è —à–ª–∞ –∫—Ä—É–≥–æ–º –æ—Ç –æ–±–∏–ª–∏—è –Ω–æ–≤—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π.

–ê—Ä—Ä–∞–∫–∏—Å ‚Äì –î—é–Ω–∞ ‚Äì –ü–ª–∞–Ω–µ—Ç–∞ –ü—É—Å—Ç—ã–Ω–∏.

–°—É—Ñ–∏—Ä –•–∞–≤–∞—Ç, —Å—Ç–∞—Ä—à–∏–π –º–∞—Å—Ç–µ—Ä-–∞—Å–∞—Å—Å–∏–Ω –ø—Ä–∏ –¥–≤–æ—Ä–µ –µ–≥–æ –æ—Ç—Ü–∞, –æ–±—ä—è—Å–Ω—è–ª –µ–º—É, —á—Ç–æ –•–∞—Ä–∫–æ–Ω–Ω–µ–Ω—ã, —Å–º–µ—Ä—Ç–µ–ª—å–Ω—ã–µ –≤—Ä–∞–≥–∏ –¥–æ–º–∞ –ê—Ç—Ä–µ–π–¥–µ—Å, –≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç –ª–µ—Ç –≤–ª–∞—Å—Ç–≤–æ–≤–∞–ª–∏ –Ω–∞–¥ –ê—Ä—Ä–∞–∫–∏—Å–æ–º ‚Äì –æ–Ω –±—ã–ª –∏—Ö –∫–≤–∞–∑–∏–ª–µ–Ω–Ω—ã–º –≤–ª–∞–¥–µ–Ω–∏–µ–º –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É –Ω–∞ –¥–æ–±—ã—á—É –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–æ–≥–æ –≥–µ—Ä–∏–∞—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å–Ω–∞–¥–æ–±—å—è, –ü—Ä—è–Ω–æ—Å—Ç–∏, –º–µ–ª–∞–Ω–∂–∏ ‚Äì –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É, –∑–∞–∫–ª—é—á–µ–Ω–Ω–æ–º—É —Å –•–∞—Ä–∫–æ–Ω–Ω–µ–Ω–∞–º–∏ –∫–æ–º–ø–∞–Ω–∏–µ–π –ö–û–û–ê–ú. –¢–µ–ø–µ—Ä—å –•–∞—Ä–∫–æ–Ω–Ω–µ–Ω—ã —É—Ö–æ–¥–∏–ª–∏, –∞ –Ω–∞ –∏—Ö –º–µ—Å—Ç–æ, –Ω–æ —É–∂–µ —Å –ø–æ–ª–Ω—ã–º –ª–µ–Ω–æ–º, –ø—Ä–∏—Ö–æ–¥–∏–ª–∏ –ê—Ç—Ä–µ–π–¥–µ—Å—ã ‚Äì –∏ –±–µ—Å—Å–ø–æ—Ä–Ω–æ—Å—Ç—å –ø–æ–±–µ–¥—ã –≥–µ—Ä—Ü–æ–≥–∞ –õ–µ—Ç–æ –ê—Ç—Ä–µ–π–¥–µ—Å–∞ –±—ã–ª–∞ –æ—á–µ–≤–∏–¥–Ω–∞. –•–æ—Ç—è‚Ä¶ –•–∞–≤–∞—Ç –µ—â–µ –≥–æ–≤–æ—Ä–∏–ª, —á—Ç–æ –≤ —Ç–∞–∫–æ–π –æ—á–µ–≤–∏–¥–Ω–æ—Å—Ç–∏ —Ç–∞–∏—Ç—Å—è —Å–º–µ—Ä—Ç–µ–ª—å–Ω–∞—è —É–≥—Ä–æ–∑–∞, –∏–±–æ –≥–µ—Ä—Ü–æ–≥ –õ–µ—Ç–æ —Å–ª–∏—à–∫–æ–º –ø–æ–ø—É–ª—è—Ä–µ–Ω –≤ –õ–∞–Ω–¥—Å—Ä–∞–∞–¥–µ –í–µ–ª–∏–∫–∏—Ö –ü—Ä–∞–≤—è—â–∏—Ö –î–æ–º–æ–≤. ¬´–ê —á—É–∂–∞—è —Å–ª–∞–≤–∞ ‚Äì –æ—Å–Ω–æ–≤–∞ –∑–∞–≤–∏—Å—Ç–∏ –≤–ª–∞–¥—ã–∫¬ª, ‚Äì —Å–∫–∞–∑–∞–ª —Ç–æ–≥–¥–∞ –•–∞–≤–∞—Ç.

–ê—Ä—Ä–∞–∫–∏—Å ‚Äì –î—é–Ω–∞ ‚Äì –ü–ª–∞–Ω–µ—Ç–∞ –ü—É—Å—Ç—ã–Ω–∏‚Ä¶

–ü–∞—É–ª—å —Å–ø–∞–ª. –ï–º—É —Å–Ω–∏–ª–∞—Å—å –∫–∞–∫–∞—è-—Ç–æ –ø–µ—â–µ—Ä–∞ –Ω–∞ –ê—Ä—Ä–∞–∫–∏—Å–µ, –º–æ–ª—á–∞–ª–∏–≤—ã–µ –ª—é–¥–∏, —Å–∫–æ–ª—å–∑—è—â–∏–µ –≤ –Ω–µ—è—Å–Ω–æ–º —Å–≤–µ—Ç–µ –ø–ª–∞–≤–∞—é—â–∏—Ö –≤ –≤–æ–∑–¥—É—Ö–µ –ª–∞–º–ø. –ò —Ç–∏—à–∏–Ω–∞ ‚Äì —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ç–∏—à–∏–Ω–∞ —Ö—Ä–∞–º–∞, –Ω–∞—Ä—É—à–∞–µ–º–∞—è —Ç–æ–ª—å–∫–æ –æ—Ç—á–µ—Ç–ª–∏–≤–æ –æ—Ç–¥–∞—é—â–∏–º–∏—Å—è –ø–æ–¥ —Å–≤–æ–¥–∞–º–∏ –∑–≤—É–∫–∞–º–∏ —á–∞—Å—Ç–æ –ø–∞–¥–∞—é—â–∏—Ö –∫–∞–ø–µ–ª—å: –∫–∞–ø-–∫–∞–ø-–∫–∞–ø‚Ä¶ –ü–∞—É–ª—å –¥–∞–∂–µ –≤ –∑–∞–±—ã—Ç—å–∏ —á—É–≤—Å—Ç–≤–æ–≤–∞–ª, —á—Ç–æ –Ω–µ –∑–∞–±—É–¥–µ—Ç —ç—Ç–æ –≤–∏–¥–µ–Ω–∏–µ ‚Äì –ø—Ä–æ–±—É–∂–¥–∞—è—Å—å, –æ–Ω –≤—Å–µ–≥–¥–∞ –ø–æ–º–Ω–∏–ª —Å–Ω—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ‚Ä¶

–í–∏–¥–µ–Ω–∏–µ —Å—Ç–∞–Ω–æ–≤–∏–ª–æ—Å—å –≤—Å–µ –±–æ–ª–µ–µ –∑—ã–±–∫–∏–º –∏ –Ω–∞–∫–æ–Ω–µ—Ü —Ä–∞—Å—Ç–∞—è–ª–æ.

–ü–∞—É–ª—å –ª–µ–∂–∞–ª –≤ –ø–æ–ª—É–¥—Ä–µ–º–µ –∏ –¥—É–º–∞–ª. –ó–∞–º–æ–∫ –ö–∞–ª–∞–¥–∞–Ω, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –Ω–µ –∑–Ω–∞–ª –∏–≥—Ä —Å–æ —Å–≤–µ—Ä—Å—Ç–Ω–∏–∫–∞–º–∏, –ø–æ–∂–∞–ª—É–π, –≤–æ–≤—Å–µ –Ω–µ –∑–∞—Å–ª—É–∂–∏–≤–∞–ª –≥—Ä—É—Å—Ç–∏ –ø—Ä–∏ —Ä–∞—Å—Å—Ç–∞–≤–∞–Ω–∏–∏. –î–æ–∫—Ç–æ—Ä –Æ–π—ç, –µ–≥–æ —É—á–∏—Ç–µ–ª—å, –Ω–∞–º–µ–∫–Ω—É–ª, —á—Ç–æ –Ω–∞ –ê—Ä—Ä–∞–∫–∏—Å–µ –∫–ª–∞—Å—Å–æ–≤—ã–µ —Ä–∞–º–∫–∏ –∫–æ–¥–µ–∫—Å–∞ –§–∞—Ñ—Ä–µ–ª–∞—Ö –Ω–µ —Å–æ–±–ª—é–¥–∞—é—Ç—Å—è —Ç–∞–∫ —Å—Ç—Ä–æ–≥–æ, –∫–∞–∫ –∑–¥–µ—Å—å. –õ—é–¥–∏ —Ç–∞–º –∂–∏–≤—É—Ç –≤ –ø—É—Å—Ç—ã–Ω–µ, –≥–¥–µ –Ω–µ—Ç –∫–∞–∏–¥–æ–≤ –∏ –±–∞—à–∞—Ä–æ–≤ –ò–º–ø–µ—Ä–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã –∫–æ–º–∞–Ω–¥–æ–≤–∞—Ç—å –∏–º–∏. –õ—é–¥–∏, –ø–æ–¥—á–∏–Ω—è—é—â–∏–µ—Å—è –ª–∏—à—å –í–æ–ª–µ –ü—É—Å—Ç—ã–Ω–∏, —Ñ—Ä–∏–º–µ–Ω—ã, ¬´–°–≤–æ–±–æ–¥–Ω—ã–µ¬ª ‚Äì –Ω–µ –≤–Ω–µ—Å–µ–Ω–Ω—ã–µ –≤ –∏–º–ø–µ—Ä—Å–∫–∏–µ –ø–µ—Ä–µ–ø–∏—Å–∏‚Ä¶

–ê—Ä—Ä–∞–∫–∏—Å ‚Äì –î—é–Ω–∞ ‚Äì –ü–ª–∞–Ω–µ—Ç–∞ –ü—É—Å—Ç—ã–Ω–∏‚Ä¶

–ü–∞—É–ª—å –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞–ª –æ—Ö–≤–∞—Ç–∏–≤—à–µ–µ –µ–≥–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–∏–ª –æ–¥–∏–Ω –∏–∑ –ø—Ä–∏–µ–º–æ–≤ –ø–æ–¥—á–∏–Ω–µ–Ω–∏—è –¥—É—Ö–∞ –∏ —Ç–µ–ª–∞, –∫–æ—Ç–æ—Ä—ã–º –Ω–∞—É—á–∏–ª–∞ –µ–≥–æ –º–∞—Ç—å. –¢—Ä–∏ –±—ã—Å—Ç—Ä—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–¥–æ—Ö–∞ ‚Äì –∏ –ø—Ä–∏–≤—ã—á–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è: –æ–Ω —Å–ª–æ–≤–Ω–æ –ø–æ–ø–ª—ã–ª, –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—è –ø—Ä–∏ —ç—Ç–æ–º —Å–≤–æ–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ ¬´—è¬ª: ‚Ä¶–∞–æ—Ä—Ç–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è‚Ä¶ —Å–æ–∑–Ω–∞–Ω–∏–µ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–æ‚Ä¶ —Å–æ–∑–Ω–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é: —è –º–æ–≥—É —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–æ–∑–Ω–∞–Ω–∏–µ–º, –≤–∫–ª—é—á–∞—Ç—å –∏ –≤—ã–∫–ª—é—á–∞—Ç—å –ø–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∂–µ–ª–∞–Ω–∏—é‚Ä¶ –º–æ—è –∫—Ä–æ–≤—å –Ω–∞—Å—ã—â–∞–µ—Ç—Å—è –∫–∏—Å–ª–æ—Ä–æ–¥–æ–º –∏ –æ–º—ã–≤–∞–µ—Ç –∏–º –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —É—á–∞—Å—Ç–∫–∏‚Ä¶ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–∏—â—É, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Å–≤–æ–±–æ–¥—É, –ø–æ–ª—å–∑—É—è—Å—å –æ–¥–Ω–∏–º –ª–∏—à—å –∏–Ω—Å—Ç–∏–Ω–∫—Ç–æ–º‚Ä¶ —Ä–∞–∑—É–º—É –∂–∏–≤–æ—Ç–Ω–æ–≥–æ –Ω–µ –¥–∞–Ω–æ –≤—ã–π—Ç–∏ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –º–æ–º–µ–Ω—Ç–∞ –∏–ª–∏ –æ—Å–æ–∑–Ω–∞—Ç—å, —á—Ç–æ –æ–Ω–æ —Å–∞–º–æ –º–æ–∂–µ—Ç —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å —Å–≤–æ—é –¥–æ–±—ã—á—É‚Ä¶ –∂–∏–≤–æ—Ç–Ω–æ–µ —Ä–∞–∑—Ä—É—à–∞–µ—Ç, –∞ –Ω–µ —Å–æ–∑–¥–∞–µ—Ç‚Ä¶ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏—è –∂–∏–≤–æ—Ç–Ω–æ–≥–æ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —á—É–≤—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –Ω–µ –≤–æ–∑–≤—ã—à–∞—è—Å—å –¥–æ –æ—Å–æ–∑–Ω–∞–Ω–∏—è‚Ä¶ —á–µ–ª–æ–≤–µ–∫ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —Å–∏—Å—Ç–µ–º–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º–∏—Ä–∞‚Ä¶ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—è —Å–æ–∑–Ω–∞–Ω–∏–µ, —è —Å–æ–∑–¥–∞—é —Ç–∞–∫—É—é —Å–∏—Å—Ç–µ–º—É‚Ä¶ –µ–¥–∏–Ω—Å—Ç–≤–æ —Ç–µ–ª–∞ —Å–ª–µ–¥—É–µ—Ç –∑–∞ —Ä–∞–±–æ—Ç–æ–π –Ω–µ—Ä–≤–Ω–æ–π –∏ –∫—Ä–æ–≤–µ–Ω–æ—Å–Ω–æ–π —Å–∏—Å—Ç–µ–º ‚Äì —Å–æ–≥–ª–∞—Å–Ω–æ –Ω—É–∂–¥–∞–º —Å–∞–º–∏—Ö –∫–ª–µ—Ç–æ–∫‚Ä¶ –≤—Å–µ —Å—É—â–µ–µ, –≤—Å–µ –ø—Ä–µ–¥–º–µ—Ç—ã, –≤—Å–µ –∂–∏–≤–æ–µ ‚Äì –≤—Å–µ –Ω–µ–ø–æ—Å—Ç–æ—è–Ω–Ω–æ‚Ä¶ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ –ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤—É –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è‚Ä¶

–°–Ω–æ–≤–∞ –∏ —Å–Ω–æ–≤–∞ –ø–æ–≤—Ç–æ—Ä—è–ª—Å—è —ç—Ç–æ—Ç —É—Ä–æ–∫ –≤ –ø–ª—ã–≤—É—â–µ–º —Å–æ–∑–Ω–∞–Ω–∏–∏ –ü–∞—É–ª—è.

–ö–æ–≥–¥–∞ –∂–µ —Å–∫–≤–æ–∑—å —à—Ç–æ—Ä—ã –ø—Ä–æ–Ω–∏–∫ –∂–µ–ª—Ç—ã–π —Å–≤–µ—Ç —É—Ç—Ä–∞, –ü–∞—É–ª—å –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞–ª –µ–≥–æ —Å–∫–≤–æ–∑—å —Å–æ–º–∫–Ω—É—Ç—ã–µ –≤–µ–∫–∏, –æ—Ç–∫—Ä—ã–ª –≥–ª–∞–∑–∞ –∏ —É—Å–ª—ã—à–∞–ª, —á—Ç–æ –≤ –∑–∞–º–∫–µ –≤–æ–∑–æ–±–Ω–æ–≤–∏–ª–∞—Å—å —Å—É–µ—Ç–∞. –£–≤–∏–¥–µ–ª –Ω–∞–¥ —Å–æ–±–æ–π –∑–Ω–∞–∫–æ–º—É—é —Ä–µ–∑—å–±—É –ø–æ—Ç–æ–ª–æ—á–Ω—ã—Ö –±–∞–ª–æ–∫‚Ä¶

–û—Ç–≤–æ—Ä–∏–ª–∞—Å—å –¥–≤–µ—Ä—å, –∏ –≤ —Å–ø–∞–ª—å–Ω—é –∑–∞–≥–ª—è–Ω—É–ª–∞ –º–∞—Ç—å: –≤–æ–ª–æ—Å—ã —Ü–≤–µ—Ç–∞ —Ç–µ–º–Ω–æ–π –±—Ä–æ–Ω–∑—ã –ø–µ—Ä–µ–≤–∏—Ç—ã —á–µ—Ä–Ω–æ–π –ª–µ–Ω—Ç–æ–π, —á–µ—Ä—Ç—ã –ª–∏—Ü–∞ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω—ã –∏ –∑–µ–ª–µ–Ω—ã–µ –≥–ª–∞–∑–∞ —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ-—Å—Ç—Ä–æ–≥–∏.

‚Äì –ü—Ä–æ—Å–Ω—É–ª—Å—è? ‚Äì —Å–ø—Ä–æ—Å–∏–ª–∞, –æ–Ω–∞. ‚Äì –•–æ—Ä–æ—à–æ –≤—ã—Å–ø–∞–ª—Å—è?

‚Äì –î–∞.

–ü–∞—É–ª—å –ø—Ä–∏—Å—Ç–∞–ª—å–Ω–æ —Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –Ω–µ–µ, –ø–æ–∫–∞ –º–∞—Ç—å –≤—ã–±–∏—Ä–∞–ª–∞ –æ–¥–µ–∂–¥—É, –ø—Ä–∏–º–µ—á–∞—è –Ω–µ–ø—Ä–∏–≤—ã—á–Ω—É—é —Å—É—Ä–æ–≤–æ—Å—Ç—å, –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –ø–ª–µ—á–∏‚Ä¶ –ù–∏–∫—Ç–æ –¥—Ä—É–≥–æ–π –Ω–µ —Ä–∞–∑–≥–ª—è–¥–µ–ª –±—ã —ç—Ç–æ–≥–æ, –Ω–æ –î–∂–µ—Å—Å–∏–∫–∞ —Å–∞–º–∞ –æ–±—É—á–∞–ª–∞ –µ–≥–æ —Ç–∞–π–Ω–∞–º –ë–µ–Ω–µ –ì–µ—Å—Å–µ—Ä–∏—Ç, –∑–∞—Å—Ç–∞–≤–ª—è–ª–∞ –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–µ–ª—å—á–∞–π—à–∏–µ –¥–µ—Ç–∞–ª–∏.
"""


def load_prompt_text(audio_path: str, instruction: str = INSTRUCTION) -> str:
    """
    Loads transcription from txt file and forms prompt_text.
    
    Format prompt_text: "{instruction}<|endofprompt|>{transcription}"
    """
    txt_path = audio_path.rsplit('.', 1)[0] + '.txt'
    
    with open(txt_path, 'r', encoding='utf-8') as f:
        transcription = f.read().strip()
    
    return f"{instruction}<|endofprompt|>{transcription}"


def apply_torch_compile(cosyvoice: FastCosyVoice3) -> None:
    """
    Applies torch.compile to LLM model for inference acceleration.
    
    Compiles the internal Qwen2ForCausalLM.model (Qwen2Model),
    which is used in forward_one_step for auto-generation.
    """
    # Path to Qwen2Model: cosyvoice.model.llm.llm.model.model
    # llm - CosyVoice3LM
    # llm.llm - Qwen2Encoder  
    # llm.llm.model - Qwen2ForCausalLM
    # llm.llm.model.model - Qwen2Model (what is actually called in forward_one_step)
    
    qwen2_model = cosyvoice.model.llm.llm.model.model
    logger.info(f"Compiling Qwen2Model: {type(qwen2_model).__name__}")
    
    compiled_model = torch.compile(qwen2_model, mode="default")
    cosyvoice.model.llm.llm.model.model = compiled_model
    
    logger.info("torch.compile applied to LLM")


def warmup_model(
    cosyvoice: FastCosyVoice3,
    prompt_text: str,
    spk_id: str,
) -> None:
    """
    Warms up the model by generating tokens to compile all execution paths.
    
    torch.compile creates different kernels for different input sizes,
    so the model needs to be warmed up on texts of different lengths.
    
    Args:
        cosyvoice: Initialized FastCosyVoice3 model
        prompt_text: Prompt text for generation
        spk_id: Speaker ID (should already be added via add_zero_shot_spk)
    """
    # Texts of different lengths to cover different input sizes
    warmup_texts = [
        # Short text (~50-100 LLM tokens)
        "Hello! How are you?",
        # Medium text (~100-200 LLM tokens)  
        "This is a test synthesis of medium-length text for model warmup.",
        # Long text (~200-400 LLM tokens)
        "This is a longer text for warmup. " * 3,
        # Very long text (~400+ LLM tokens)
        "Warming up the model on a long text for compilation. " * 5,
    ]
    
    warmup_start = time.time()
    
    # First pass - main compilation
    logger.info("Warmup: first pass (kernel compilation)...")
    for i, text in enumerate(warmup_texts):
        logger.info(f"  Warmup text {i+1}/{len(warmup_texts)}: {len(text)} characters")
        for _ in cosyvoice.inference_zero_shot(
            tts_text=text,
            prompt_text=prompt_text,
            prompt_wav=REFERENCE_AUDIO,
            zero_shot_spk_id=spk_id,
        ):
            pass  # Just generate all segments
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Second pass - ensure all paths are compiled
    logger.info("Warmup: second pass (stabilization)...")
    for text in warmup_texts:
        for _ in cosyvoice.inference_zero_shot(
            tts_text=text,
            prompt_text=prompt_text,
            prompt_wav=REFERENCE_AUDIO,
            zero_shot_spk_id=spk_id,
        ):
            pass
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    warmup_time = time.time() - warmup_start
    logger.info(f"Warmup completed in {warmup_time:.2f} sec")


def synthesize(
    cosyvoice: FastCosyVoice3,
    text: str,
    prompt_text: str,
    spk_id: str,
    sample_rate: int,
    output_path: str,
    speed: float = 1.0
) -> dict:
    """
    Performs non-streaming synthesis of text and returns metrics.
    
    Args:
        cosyvoice: FastCosyVoice3 model
        text: Text for synthesis
        prompt_text: Reference audio transcription
        spk_id: Speaker ID
        sample_rate: Sample rate
        output_path: Path to save the result
        speed: Speech speed multiplier (1.0 = normal)
    
    Returns:
        dict with keys: total_time, audio_duration, rtf, segment_count
    """
    start_time = time.time()
    audio_segments: list[bytes] = []
    segment_count = 0

    infer_ctx = torch.inference_mode() if USE_INFERENCE_MODE else torch.no_grad()
    with infer_ctx:
        for pcm_bytes in cosyvoice.inference_zero_shot(
            tts_text=text,
            prompt_text=prompt_text,
            prompt_wav=REFERENCE_AUDIO,
            zero_shot_spk_id=spk_id,
            speed=speed,
        ):
            segment_count += 1
            audio_segments.append(pcm_bytes)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    total_time = time.time() - start_time
    
    # Concatenate segments and save as WAV
    if audio_segments:
        full_pcm = b''.join(audio_segments)
        # Save as WAV (PCM int16, mono)
        with wave.open(output_path, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 16-bit = 2 bytes
            wf.setframerate(sample_rate)
            wf.writeframes(full_pcm)
        # Calculate duration from PCM bytes (2 bytes per sample, mono)
        audio_duration = len(full_pcm) / 2 / sample_rate
    else:
        audio_duration = 0.0
    
    rtf = total_time / audio_duration if audio_duration > 0 else float('inf')
    
    return {
        'total_time': total_time,
        'audio_duration': audio_duration,
        'rtf': rtf,
        'segment_count': segment_count,
    }


def main():
    print("=" * 70)
    print("FastCosyVoice3 TTS - Non-streaming (Offline) Inference")
    print("=" * 70)
    
    # Check for reference audio
    if not os.path.exists(REFERENCE_AUDIO):
        logger.error(f"Reference audio not found: {REFERENCE_AUDIO}", exc_info=True)
        return
    
    # Create output directory
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    
    # Load prompt_text from txt file next to audio
    prompt_text = load_prompt_text(REFERENCE_AUDIO, INSTRUCTION)
    
    print(f"\nüé§ Reference audio: {REFERENCE_AUDIO}")
    print(f"üìù Text for synthesis: {SYNTHESIS_TEXT[:80]}{'...' if len(SYNTHESIS_TEXT) > 80 else ''}")
    
    # Load model with parallel pipeline and TensorRT
    print("\nüîß Loading FastCosyVoice3...")
    print(f"   - TensorRT Flow: {'‚úÖ' if USE_TRT_FLOW else '‚ùå'}")
    print(f"   - TensorRT-LLM:  {'‚úÖ' if USE_TRT_LLM else '‚ùå'} (dtype={TRT_LLM_DTYPE})")
    
    load_start = time.time()
    
    cosyvoice = FastCosyVoice3(
        model_dir=MODEL_DIR,
        fp16=True,
        load_trt=USE_TRT_FLOW,       # TensorRT for Flow decoder (~2.5x speedup)
        load_trt_llm=USE_TRT_LLM,    # TensorRT-LLM for LLM (~3x speedup)
        trt_llm_dtype=TRT_LLM_DTYPE,
        trt_llm_kv_cache_tokens=TRT_LLM_KV_CACHE_TOKENS,
    )
    
    load_time = time.time() - load_start
    print(f"‚úÖ Model loaded in {load_time:.2f} sec")
    
    if USE_TRT_LLM and cosyvoice.trt_llm_loaded:
        print("‚úÖ TensorRT-LLM loaded successfully")
    elif USE_TRT_LLM:
        print("‚ö†Ô∏è TensorRT-LLM not loaded, using PyTorch")
    
    # dtype diagnostics
    llm_dtype = next(cosyvoice.model.llm.parameters()).dtype
    flow_dtype = next(cosyvoice.model.flow.parameters()).dtype
    hift_dtype = next(cosyvoice.model.hift.parameters()).dtype
    print(f"üìä LLM dtype: {llm_dtype}, Flow dtype: {flow_dtype}, HiFT dtype: {hift_dtype}")
    
    sample_rate = cosyvoice.sample_rate
    print(f"üìä Sample rate: {sample_rate} Hz")
    
    # Parallel pipeline information
    print("\nüöÄ Inference mode: Non-streaming (offline)")
    if USE_TRT_LLM and cosyvoice.trt_llm_loaded:
        print("   - LLM: TensorRT-LLM (~3x speedup)")
    else:
        print("   - LLM: PyTorch + torch.compile")
    if USE_TRT_FLOW:
        print("   - Flow: TensorRT (~2.5x speedup)")
    else:
        print("   - Flow: PyTorch")
    print("   - Hift: PyTorch (f0_predictor on CPU)")
    
    # Apply torch.compile to LLM only if TRT-LLM is not used
    if not (USE_TRT_LLM and cosyvoice.trt_llm_loaded):
        print("\n‚ö° Applying torch.compile to LLM...")
        compile_start = time.time()
        apply_torch_compile(cosyvoice)
        compile_time = time.time() - compile_start
        print(f"‚úÖ torch.compile applied in {compile_time:.3f} sec")
    else:
        print("\n‚ö° torch.compile skipped (using TensorRT-LLM)")
    
    # Prepare speaker embeddings (once)
    print("\nüéØ Preparing speaker embeddings...")
    spk_id = "reference_speaker"
    embed_start = time.time()
    cosyvoice.add_zero_shot_spk(prompt_text, REFERENCE_AUDIO, spk_id)
    embed_time = time.time() - embed_start
    print(f"‚úÖ Embeddings prepared in {embed_time:.3f} sec")
    
    # Model warmup
    if USE_TRT_LLM and cosyvoice.trt_llm_loaded:
        # With TRT-LLM warmup is shorter - only Flow and Hift
        print("\nüî• Warming up model (TRT-LLM doesn't require long warmup)...")
        for _ in cosyvoice.inference_zero_shot(
            tts_text="Short model warmup.",
            prompt_text=prompt_text,
            prompt_wav=REFERENCE_AUDIO,
            zero_shot_spk_id=spk_id,
        ):
            pass
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        print("‚úÖ Model warmed up")
    else:
        # Without TRT-LLM full warmup is needed for torch.compile
        print("\nüî• Warming up model (compiling graphs for different text lengths)...")
        warmup_model(cosyvoice, prompt_text, spk_id)
        print("‚úÖ Model warmed up and ready")
    
    # Generate text
    print("\n" + "=" * 70)
    print("üìÑ Generating audio")
    print("=" * 70)
    print(f"üìù {SYNTHESIS_TEXT[:80]}{'...' if len(SYNTHESIS_TEXT) > 80 else ''}")
    
    output_file = os.path.join(OUTPUT_DIR, 'output.wav')
    
    try:
        metrics = synthesize(
            cosyvoice=cosyvoice,
            text=SYNTHESIS_TEXT,
            prompt_text=prompt_text,
            spk_id=spk_id,
            sample_rate=sample_rate,
            output_path=output_file,
        )
        
        print(f"\nüíæ Saved: {output_file}")
        print("\nüìä METRICS:")
        print("-" * 40)
        print(f"‚è±Ô∏è  Total time:       {metrics['total_time']:.3f} sec")
        print(f"üéµ Duration:         {metrics['audio_duration']:.3f} sec")
        print(f"üìà RTF:              {metrics['rtf']:.3f}")
        print(f"üì¶ Segments:         {metrics['segment_count']}")
        
        if metrics['rtf'] < 1.0:
            print(f"‚úÖ Faster than real-time by {1/metrics['rtf']:.1f}x")
        else:
            print(f"‚ö†Ô∏è  Slower than real-time by {metrics['rtf']:.1f}x")
        
        # Final summary
        print("\n" + "=" * 70)
        print("üìä SUMMARY (FastCosyVoice3 - Non-streaming)")
        print("=" * 70)
        
        # Configuration
        llm_backend = "TensorRT-LLM" if (USE_TRT_LLM and cosyvoice.trt_llm_loaded) else "PyTorch+torch.compile"
        flow_backend = "TensorRT" if USE_TRT_FLOW else "PyTorch"
        print(f"LLM:  {llm_backend}")
        print(f"Flow: {flow_backend}")
        print("-" * 40)
        
        print(f"RTF:                 {metrics['rtf']:.3f}")
        print(f"Audio duration:      {metrics['audio_duration']:.3f} sec")
        print(f"Total time:          {metrics['total_time']:.3f} sec")
        
        if metrics['rtf'] < 1.0:
            print(f"\n‚úÖ Speed: {1/metrics['rtf']:.1f}x faster than real-time")
            
    except Exception as e:
        logger.error(f"Error synthesizing text: {e}", exc_info=True)
        return
    
    # Attempt to free temporary PyTorch buffers
    # Important: KV-cache TensorRT-LLM and TensorRT workspace are not freed this way
    # (they live as long as the runner/engine lives).
    try:
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    except Exception as e:
        logger.error(f"Error clearing memory: {e}", exc_info=True)
    
    print("\n" + "=" * 70)
    print("‚úÖ GENERATION COMPLETED!")
    print("=" * 70)
    print(f"\nüìÅ Results: {OUTPUT_DIR}/")


if __name__ == '__main__':
    main()

